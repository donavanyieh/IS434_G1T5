{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473fd326",
   "metadata": {},
   "source": [
    "# IMPORT & INITIALISATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f11785e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in c:\\users\\kytan\\anaconda3\\lib\\site-packages (5.8.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en\n",
    "# !pip install plotly==5.6.0\n",
    "# !pip install dash\n",
    "# !pip install \"jupyterlab>=3\" \"ipywidgets>=7.6\"\n",
    "# !pip install chart-studio==1.1.0\n",
    "# !pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a15d529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77377c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    temp = text.lower()\n",
    "    temp = re.sub(\"'\", \"\", temp) # to avoid removing contractions in english\n",
    "    temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "    temp = re.sub(r'http\\S+', '', temp)\n",
    "    temp = re.sub('[()!?]', ' ', temp)\n",
    "    temp = re.sub('\\[.*?\\]',' ', temp)\n",
    "    temp = re.sub(\"[^a-z0-9]\",\" \", temp)\n",
    "#     temp = temp.split()\n",
    "#     temp = [w for w in temp if not w in stopwords]\n",
    "#     temp = \" \".join(word for word in temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "358a1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get unique words for each aspect \n",
    "def unique_words(aspect_dict, aspect): \n",
    "    unique_words_dict = {}\n",
    "    unique_word_list = []\n",
    "    for desc in aspect_dict[aspect]:\n",
    "        if desc not in unique_word_list:\n",
    "            unique_word_list.append(desc)\n",
    "        \n",
    "    return unique_word_list\n",
    "\n",
    "#get the count for each unique word for each aspect \n",
    "def get_counts_desc_list(aspect_dict, aspect):\n",
    "    count_desc = {}\n",
    "    count_list = []\n",
    "\n",
    "    \n",
    "    for desc in aspect_dict[aspect]:\n",
    "        if desc not in count_desc:\n",
    "            count_desc[desc] = 1\n",
    "        else:\n",
    "            count_desc[desc] += 1 \n",
    "    \n",
    "    for desc in count_desc:\n",
    "        count_list.append(count_desc[desc])\n",
    "\n",
    "                        \n",
    "    \n",
    "    return count_list \n",
    "\n",
    "\n",
    "def get_proportion_desc_list(aspect_dict,aspect):\n",
    "    count_desc = {}\n",
    "    count_list = []\n",
    "    proportion_list = []\n",
    "    total= 0 \n",
    "    \n",
    "    \n",
    "    for desc in aspect_dict[aspect]:\n",
    "        if desc not in count_desc:\n",
    "            count_desc[desc] = 1\n",
    "        else:\n",
    "            count_desc[desc] += 1 \n",
    "    \n",
    "    for desc in count_desc:\n",
    "        total += count_desc[desc]\n",
    "    for desc in count_desc:\n",
    "        proportion_list.append(count_desc[desc]/total)\n",
    "\n",
    "                        \n",
    "    \n",
    "    return proportion_list \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbfda769",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c2356",
   "metadata": {},
   "source": [
    "# GETTING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f914fa4",
   "metadata": {},
   "source": [
    "###### MENTAIYA: \n",
    "pos_reviews1 = pd.read_excel(r\"C:\\Users\\kytan\\OneDrive\\Desktop\\SMUACADS\\Y2S2\\SA\\Projects\\github\\IS434_G1T5\\data\\Competitorsdata_ABSA\\mentaiya_cleaned_pos_reviews.xlsx\", names=['Reviews'])\n",
    "neg_reviews1 = pd.read_excel(r\"C:\\Users\\kytan\\OneDrive\\Desktop\\SMUACADS\\Y2S2\\SA\\Projects\\github\\IS434_G1T5\\data\\Competitorsdata_ABSA\\mentaiya_cleaned_neg_reviews.xlsx\", names=[\"Reviews\"])\n",
    "\n",
    "###### FUKUDON\n",
    "\n",
    "pos_reviews1= pd.read_excel(r\"C:\\Users\\kytan\\OneDrive\\Desktop\\SMUACADS\\Y2S2\\SA\\Projects\\github\\IS434_G1T5\\data\\Competitorsdata_ABSA\\fukudon_cleaned_pos_reviews.xlsx\", names=['Reviews'])\n",
    "\n",
    "neg_reviews1= pd.read_excel(r\"C:\\Users\\kytan\\OneDrive\\Desktop\\SMUACADS\\Y2S2\\SA\\Projects\\github\\IS434_G1T5\\data\\Competitorsdata_ABSA\\fukudon_cleaned_neg_reviews.xlsx\", names=['Reviews'])\n",
    "\n",
    "\n",
    "###### STICKSNBOWLS \n",
    "pos_reviews1 = pd.read_csv(r\"C:\\Users\\kytan\\OneDrive\\Desktop\\SMUACADS\\Y2S2\\SA\\Projects\\github\\IS434_G1T5\\data\\Competitorsdata_ABSA\\stickNbowls_cleaned_pos_reviews.csv\", names=['Reviews'])\n",
    "neg_reviews1 = pd.read_csv(r\"C:\\Users\\kytan\\OneDrive\\Desktop\\SMUACADS\\Y2S2\\SA\\Projects\\github\\IS434_G1T5\\data\\Competitorsdata_ABSA\\stickNbowls_cleaned_neg_reviews.csv\", names=[\"Reviews\"]) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9e4a2e-bde8-4b88-8756-1def60f900fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MENTAIYA\n",
    "pos_reviews1 = pd.read_excel(r\"C:\\Users\\kytan\\OneDrive\\Desktop\\SMUACADS\\Y2S2\\SA\\Projects\\Competitorsdata\\mentaiya_cleaned_pos_reviews.xlsx\", names=['Reviews'])\n",
    "neg_reviews1 = pd.read_excel(r\"C:\\Users\\kytan\\OneDrive\\Desktop\\SMUACADS\\Y2S2\\SA\\Projects\\Competitorsdata\\mentaiya_cleaned_neg_reviews.xlsx\", names=[\"Reviews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "80f1f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STICKSNBOWLS \n",
    "pos_reviews1 = pd.read_csv(r\"C:\\Users\\kytan\\OneDrive\\Desktop\\SMUACADS\\Y2S2\\SA\\Projects\\Competitorsdata\\stickNbowls_cleaned_pos_reviews.csv\", names=['Reviews'])\n",
    "neg_reviews1 = pd.read_csv(r\"C:\\Users\\kytan\\OneDrive\\Desktop\\SMUACADS\\Y2S2\\SA\\Projects\\Competitorsdata\\stickNbowls_cleaned_neg_reviews.csv\", names=[\"Reviews\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "626469b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUKUDON\n",
    "pos_reviews1= pd.read_excel(r\"C:\\Users\\kytan\\OneDrive\\Desktop\\SMUACADS\\Y2S2\\SA\\Projects\\Competitorsdata\\fukudon_cleaned_pos_reviews.xlsx\", names=['Reviews'])\n",
    "neg_reviews1= pd.read_excel(r\"C:\\Users\\kytan\\OneDrive\\Desktop\\SMUACADS\\Y2S2\\SA\\Projects\\Competitorsdata\\fukudon_cleaned_neg_reviews.xlsx\", names=['Reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1952ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove repeats for each df in case have\n",
    "pos_reviews = pos_reviews1.drop_duplicates(\"Reviews\", inplace = False)\n",
    "pos_reviews.reset_index(inplace=True)\n",
    "neg_reviews = neg_reviews1.drop_duplicates(\"Reviews\")\n",
    "neg_reviews.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b22c1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_reviews= pos_reviews.drop(columns=\"index\")\n",
    "neg_reviews = neg_reviews.drop(columns=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6b877-2cdc-4f54-b1c3-8cec7c7c7d07",
   "metadata": {},
   "source": [
    "# POSITIVE REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd013df7-f301-42b2-98a9-54824b24ada9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>ordered company gathering food came hot fresh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>pork n chucky fries nicer n fries highly recom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>good food better previous stall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>food prices affordable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>run good guys alliance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Reviews\n",
       "70  ordered company gathering food came hot fresh ...\n",
       "71  pork n chucky fries nicer n fries highly recom...\n",
       "72                    good food better previous stall\n",
       "73                             food prices affordable\n",
       "74                             run good guys alliance"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_reviews.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6895142-daa4-4116-a0df-81b1f2c04f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUT INTO A LIST \n",
    "pos_reviews_list = pos_reviews[\"Reviews\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "645980da-172b-4779-8ff5-753ef0c1637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_aspect_list = []\n",
    "\n",
    "for sentence in pos_reviews_list:\n",
    "    s = clean_text(sentence)\n",
    "    doc = nlp(s)\n",
    "    descriptive_term = ''\n",
    "    aspects = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ' :\n",
    "            descriptive_term = token\n",
    "        if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "            aspects.append(str(token))\n",
    "    pos_aspect_list += aspects\n",
    "    \n",
    "            \n",
    "#     print(\"===========================================\")\n",
    "#     print(\"************** Aspects **************\")\n",
    "#     print(\", \".join(aspects)) #get the noun of sentence\n",
    "#     print(\"************** Description ***************\")\n",
    "#     print(descriptive_term)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b0dc3d9-23c6-48fa-bf85-4f0be37fdff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'food': 36, 'pork': 16, 'sauce': 11, 'price': 10, 'belly': 9, 'katsu': 9, 'portion': 9, 'hawker': 9, 'curry': 8, 'rice': 7, 'love': 7, 'tender': 7, 'tasty': 7, 'beef': 7, 'fries': 7, 'egg': 6, 'time': 6, 'chicken': 6, 'stall': 6, 'donburi': 6, 'delivery': 5, 'karaage': 5, 't': 5, 'taste': 5, 'salad': 4, 'spicy': 4, 'collar': 4, 'crispy': 4, 'order': 4, 'bowl': 4, 'service': 3, 'punggol': 3, 'platter': 3, 'onsen': 3, 'try': 3, 'dish': 3, 'guys': 3, 'eggs': 3, 'onion': 3, 'texture': 3, 'thunder': 3, 'crunchy': 3, 'place': 3, 'prices': 3, 'delicious': 3, 'area': 3, 'queue': 3, 'fukudon': 3, 'quality': 3, 'salmon': 3, 'restaurant': 3, 'portions': 2, 'good': 2, 'jap': 2, 'ingredients': 2, 'bite': 2, 'mention': 2, 'gem': 2, 'marine': 2, 'parade': 2, 'meat': 2, 'chunks': 2, 'tomatoes': 2, 'cucumber': 2, 'goma': 2, 'components': 2, 'flavour': 2, 'mentaiko': 2, 'center': 2, 'garlic': 2, 'w': 2, 'homemade': 2, 'amount': 2, 'm': 2, 'support': 2, 'sukiyaki': 2, 'store': 2, 'plate': 2, 'tonkatsu': 2, 'items': 2, 'friends': 2, 'chef': 2, 'remus': 2, 'tomato': 2, 'menu': 2, 'hawkers': 2, 'karage': 2, 'potato': 2, 'value': 2, 'karrage': 2, 'surprise': 1, 'kptpeople': 1, 'counter': 1, 'point': 1, 'groupbuy': 1, 'communication': 1, 'well': 1, 'pplawesome': 1, 'pricingnot': 1, 'style': 1, 'colleagues': 1, 'lunch': 1, 'choice': 1, 'yummzzz': 1, 'wcurry': 1, 'ingredient': 1, 'yummy': 1, 'cuisines': 1, 'touch': 1, 'photo': 1, 'doesn': 1, 'justice': 1, 'char': 1, 'sauces': 1, 'market': 1, 'stars': 1, 'crust': 1, 'glistening': 1, 'piece': 1, 'pity': 1, 'fatty': 1, 'lean': 1, 'howeverii': 1, 'belowburppletakeawaystaste': 1, 'japanesestyle': 1, 'strings': 1, 'rawness': 1, 'segments': 1, 'mandarin': 1, 'savoury': 1, 'furikake': 1, 'finish': 1, 'depth': 1, 'offer': 1, 'execution': 1, 'visit': 1, 'powder': 1, 'hint': 1, 'spiciness': 1, 'tickle': 1, 'tastebuds': 1, 'element': 1, 'deal': 1, 'runny': 1, 'dashi': 1, 'broth': 1, 'onions': 1, 'silky': 1, 'generation': 1, 'venture': 1, 'industry': 1, 'attitude': 1, 'shop': 1, 'realise': 1, 'operating': 1, 'hours': 1, 'monday': 1, 'soso': 1, 'hand': 1, 'tastes': 1, 'mcdonald': 1, 'chance': 1, 'buy': 1, 'dons': 1, 'japanese': 1, 'centre': 1, 'effort': 1, 'youngsters': 1, 'badi': 1, 'line': 1, 'longhappy': 1, 'entrepreneurs': 1, 'complimentary': 1, 'miso': 1, 'soup': 1, 'potatoey': 1, 'great': 1, 'goto': 1, 'wait': 1, 'pax': 1, 'stuff': 1, 'end': 1, 'bowls': 1, 'name': 1, 'crispness': 1, 'dip': 1, 'comfort': 1, 'months': 1, 'dollar': 1, 'people': 1, 'clock': 1, 'reviews': 1, 'cause': 1, 'kudos': 1, 'girl': 1, 'takeaway': 1, 'recommend': 1, 'kitchen': 1, 'personallychicken': 1, 'oranges': 1, 'combo': 1, 'delicioustheir': 1, 'orders': 1, 'fee': 1, 'islandwidemenu': 1, 'blackboard': 1, 'special': 1, 'satay': 1, 'achar': 1, 'floss': 1, 'opening': 1, 'outlet': 1, 'hood': 1, 'servings': 1, 'family': 1, 'standard': 1, 'worth': 1, 'home': 1, 'team': 1, 'moist': 1, 'meal': 1, 'court': 1, 'proprietors': 1, 'salads': 1, 'share': 1, 'masses': 1, 'disappointmentoveralls': 1, 'money': 1, 'asian': 1, 'fuision': 1, 'passion': 1, 'flavours': 1, 'happiness': 1, 'eatswill': 1, 'update': 1, 'detail': 1, 'success': 1, 'ordered': 1, 'oyakodon': 1, 'heartlands': 1, 'dishes': 1, 'buds': 1, 'variety': 1, 'venue': 1, 'cravings': 1, 'wagyo': 1, 'onigiri': 1, 'melts': 1, 'mouth': 1, 'shoyu': 1, 'unique': 1, 'orange': 1, 'pieces': 1, 'staff': 1, 'outer': 1, 'company': 1, 'phone': 1, 'call': 1, 'island': 1, 'gathering': 1, 'chucky': 1, 'alliance': 1})\n"
     ]
    }
   ],
   "source": [
    "# COUNTING ITEMS \n",
    "pos_freq = collections.Counter(pos_aspect_list)\n",
    "print(pos_freq)\n",
    "# # print(\"#######################################################################################################################\")\n",
    "# top_pos= dict(collections.Counter(pos_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5fe1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pos_aspects = []\n",
    "top_pos =dict(pos_freq)\n",
    "for key in top_pos:\n",
    "    if top_pos[key] >= (len(pos_reviews)*0.10):\n",
    "        print(key)\n",
    "        top_pos_aspects.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "150455b1-a5b8-4f5e-8a0e-bb7c79fc87dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food', 'delivery', 'pork', 'belly', 'katsu', 'rice', 'egg', 'portion', 'time', 'hawker', 'love', 'chicken', 'karaage', 'sauce', 't', 'tender', 'tasty', 'salad', 'stall', 'beef', 'fries', 'curry', 'spicy', 'donburi', 'collar', 'price', 'crispy', 'taste', 'order', 'bowl']\n"
     ]
    }
   ],
   "source": [
    "print(top_pos_aspects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a357f2d-94a2-481f-b1ba-47f20b1442a6",
   "metadata": {},
   "source": [
    "### COMPILING INTO DF - POSITIVE \n",
    "\n",
    "#### Two types of DF created: \n",
    "1. Sentence --> Aspect (purpose is to see which sentence pointed to the aspects)\n",
    "2. Aspect --> Descriptive words (Purpose is to see what each aspect was described)\n",
    "\n",
    "We chose to use the second one for our analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab0f81-e958-47b1-af27-08169a3555eb",
   "metadata": {},
   "source": [
    "### 1.Create a dictionary that points sentence to aspect then make into a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fae984b0-3d95-411e-b179-45ff96d19b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a sentence-aspect dictionary \n",
    "pos_sent_aspect_dict = {}\n",
    "for sentence in pos_reviews_list:\n",
    "    s = clean_text(sentence)\n",
    "    doc = nlp(s)\n",
    "    descriptive_term = ''\n",
    "    aspects = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ' :\n",
    "            descriptive_term = token\n",
    "        if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" or token.pos_ ==\"VERB\":\n",
    "            aspects.append(str(token)) #for each sentence, store all the aspects in a list \n",
    "            if str(token) in top_pos_aspects:\n",
    "                pos_sent_aspect_dict[s] = str(token)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "526134d1-0f7e-4691-8b2e-339c8c3f63e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame.from_dict(pos_sent_aspect_dict, orient='index', columns=['Aspect'])\n",
    "pos_df.to_csv(\"pos_sent_asp_fukudon.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21ef52-de15-4bb4-b0c1-ccfc87ca59a7",
   "metadata": {},
   "source": [
    "### 2. Create a dict then df for aspect-descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e582031b-808c-4e6c-8a14-9c5b4169a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aim to create a asepct-descriptive term df \n",
    "\n",
    "# initialisation\n",
    "temp_pos_asp_descrp_dict = {}\n",
    "asp = ''\n",
    "# code \n",
    "for sentence in pos_reviews_list:\n",
    "    s = clean_text(sentence)\n",
    "    doc = nlp(s)\n",
    "    descriptive_term = ''\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ' :\n",
    "            descriptive_term = str(token)\n",
    "        if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "            asp = str(token)\n",
    "        \n",
    "        if asp in top_pos_aspects:\n",
    "            if descriptive_term != '':\n",
    "                if asp not in temp_pos_asp_descrp_dict:\n",
    "                    temp_pos_asp_descrp_dict[asp] = descriptive_term\n",
    "                else:\n",
    "                    temp_pos_asp_descrp_dict[asp] += \", \" + descriptive_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c436ec2c-2f5b-4bff-9d8a-2f281398ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_aspect_descrp_df = pd.DataFrame.from_dict(temp_pos_asp_descrp_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "491454ae-ab97-4fbf-90b7-ebbaf1a2f8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                          0\n",
      "food      delicious, prompt, japanese, less, awesome, aw...\n",
      "delivery  prompt, excellent, awesome, free, minimum, fre...\n",
      "pork      friendlyordered, expected, impressed, complica...\n",
      "belly           friendlyordered, expected, impressed, super\n",
      "katsu     friendlyordered, friendlyordered, delicious, i...\n",
      "rice      extra, good, plentiful, cheap, cheap, cheap, c...\n",
      "egg       extra, less, less, complicating, short, genero...\n",
      "portion   hot, hot, generous, worth, good, double, good,...\n",
      "time      affordable, best, soggy, soggy, expected, next...\n",
      "hawker    best, central, younger, particular, humble, fa...\n",
      "love                             good, healthy, donburimade\n",
      "chicken      good, complicating, light, double, good, inner\n",
      "karaage        good, complicating, pricey, sexy, sexy, sexy\n",
      "sauce     good, least, least, least, plentiful, japanese...\n",
      "tender    expected, marinated, unctuous, interested, lig...\n",
      "tasty     expected, soft, scrambled, affordable, long, d...\n",
      "fries     interested, interested, interested, homemade, ...\n",
      "curry     homemade, complicating, pricey, light, light, ...\n",
      "spicy                  japanese, complicating, complicating\n",
      "donburi   top, slight, decent, decent, good, cheap, sexy...\n",
      "collar                  complicating, complicating, diverse\n",
      "price     complicating, complicating, pricey, cheap, wor...\n",
      "crispy                            light, sexy, super, super\n",
      "stall     humble, long, long, prepared, patronising, pre...\n",
      "t           good, diverse, diverse, fantastic, itcan, itcan\n",
      "taste     good, good, cheap, nice, good, delightful, int...\n",
      "beef                             short, short, super, short\n",
      "bowl      short, short, short, short, short, short, shor...\n",
      "order             minimum, tasty, tasty, tasty, itcan, next\n",
      "salad        healthy, healthy, healthy, fresh, super, cheap\n"
     ]
    }
   ],
   "source": [
    "# uncomment if you want to visualise to see output \n",
    "print(pos_aspect_descrp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14a7b8f",
   "metadata": {},
   "source": [
    "### POSITIVE VISUALISATION\n",
    "Use plotly:https://plotly.com/python/getting-started/ \n",
    "\n",
    "For each aspect, get unique description words (x-axis) and the frequency (y-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46bf8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put description as a list for each aspect in top_5_neg_aspects:\n",
    "pos_asp_dict = {}\n",
    "for asp in temp_pos_asp_descrp_dict:\n",
    "    content = temp_pos_asp_descrp_dict[asp]\n",
    "    to_list = content.split(',')\n",
    "    pos_asp_dict[asp] = to_list\n",
    "\n",
    "# #combine 2 similar words\n",
    "# pos_asp_dict['mentaiko'] += pos_asp_dict['mentai']\n",
    "# del pos_asp_dict['mentai'] #delete the one added "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d4fafd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# BAR CHART \n",
    "for asp in pos_asp_dict:\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for x,y in zip(unique_words(pos_asp_dict,asp),get_counts_desc_list(pos_asp_dict, asp)):\n",
    "        if y>1:\n",
    "            x_list.append(x)\n",
    "            y_list.append(y)\n",
    "    if len(x_list)!=0:\n",
    "        fig = px.bar(x= x_list, y= y_list, title=\"Positive: \" + asp, labels=dict(x =\"Description\",y= \"Frequency\"))\n",
    "        fig.update_layout(xaxis={'categoryorder':'total descending'})\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6273aea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PIE CHART\n",
    "for asp in pos_asp_dict:\n",
    "    df = pd.DataFrame(unique_words(pos_asp_dict,asp), columns=['Description'])\n",
    "    df['counts'] = get_counts_desc_list(pos_asp_dict, asp)\n",
    "    fig = px.pie(df, values= 'counts', names='Description', title='Positive: '+ asp)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e413cc1c-f692-46d4-ae3b-f059f8253fa4",
   "metadata": {},
   "source": [
    "# NEGATIVE REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "14953717-e621-463e-b0ab-906e33e53a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_reviews_list = neg_reviews[\"Reviews\"].to_list()  #convert to list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "3a407f8d-4577-4ad7-83a5-93d015bccb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_aspect_list = []\n",
    "neg_appended = []\n",
    "# sentence_list = []\n",
    "# a_list = []\n",
    "for sentence in neg_reviews_list:\n",
    "    s = clean_text(sentence)\n",
    "    doc = nlp(s)\n",
    "    descriptive_term = ''\n",
    "    aspects = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ' :\n",
    "            descriptive_term = token\n",
    "        if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "            aspects.append(str(token))\n",
    "    neg_appended.append(aspects)        \n",
    "\n",
    "    neg_aspect_list += aspects\n",
    "    \n",
    "\n",
    "# UNCOMMENT THE FOLLOWING TO SEE ASPECTS AND DESCRIPTION OUTPUTS\n",
    "            \n",
    "#     print(\"===========================================\")\n",
    "#     print(\"************** Aspects **************\")\n",
    "#     print(\", \".join(aspects)) #get the noun of sentence\n",
    "#     print(\"************** Description ***************\")\n",
    "#     print(descriptive_term)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd40889b",
   "metadata": {},
   "source": [
    "##### FOR COUNTING OF ITEMS:\n",
    "Set line 5 to 0.10 to make sure that the aspects gathered appear at least 10% of all the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "94166476-f9b3-4e14-8e6f-9d1371a7093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTING ITEMS \n",
    "neg_freq = collections.Counter(neg_aspect_list)\n",
    "top_neg_aspects = []\n",
    "top_neg_dict =dict(neg_freq)\n",
    "for key in top_neg_dict:\n",
    "    if top_neg_dict[key] >= (len(neg_reviews)*0.10):\n",
    "        top_neg_aspects.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1809ae97-d0bd-4258-a589-2ce5abe124a5",
   "metadata": {},
   "source": [
    "### COMPILING INTO DF - NEGATIVE\n",
    "\n",
    "#### Two types of DF created: \n",
    "1. Sentence --> Aspect (purpose is to see which sentence pointed to the aspects)\n",
    "2. Aspect --> Descriptive words (Purpose is to see what each aspect was described)\n",
    "\n",
    "We chose to use the second one for our analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8e24c-7c84-430d-8f2d-7a48bf995304",
   "metadata": {},
   "source": [
    "### 1.Create a dictionary that points sentence to aspect then make into a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "27d097be-725e-4518-90cf-6eab6ad604de",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sent_aspect_dict = {}\n",
    "for sentence in neg_reviews_list:\n",
    "    s = clean_text(sentence)\n",
    "    doc = nlp(s)\n",
    "    descriptive_term = ''\n",
    "    aspects = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ' :\n",
    "            descriptive_term = token\n",
    "        if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\":\n",
    "            aspects.append(str(token)) #for each sentence, store all the aspects in a list \n",
    "            if str(token) in top_neg_aspects:\n",
    "#                 if str(token) not in sent_aspect_dict:\n",
    "                neg_sent_aspect_dict[s] = str(token)\n",
    "#                 else:\n",
    "#                     sent_aspect_dict[str(token)] += s \n",
    "#     neg_appended.append(aspects)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "d1df49fd-e2f1-4c36-90cf-8a203c9465d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df = pd.DataFrame.from_dict(neg_sent_aspect_dict, orient='index', columns=['Aspect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a54baae4-9fd8-4aac-9e4e-4877e233c2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>super valued money super affordable yet restaurant standard sukiyaki beef don  returning weekly satisfy cravings yes friendly staffs too</th>\n",
       "      <td>beef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place certainly worthy tried pork katsu beef fell way expectations katsu soggy beef toughhad marine parade outletpunggol outlet might better</th>\n",
       "      <td>outlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>picture doesn t food justice fav place pork spicy super fragrant super appetising would crave sometimes tried dons including salmon chicken oyakodon katsu really rather worth price point salmon offered tomato salad good katsu might get abit soggy longer crispy would reco going takeaway</th>\n",
       "      <td>takeaway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marine parade saw stall selling japanese constant queue decided try person took order wrote slip paper order order number gave one slip slip person preparing food cashier would shout order number order done would advise ordering get seats near stall know food readyi got teriyaki beef short plate w onsen egg consists beef short plate onions onsen egg furikakeso think dish come onions either minced onions really fine rice rice bad drenched beef drippings added flavour plain rice beef tender coated teriyaki sauce topped furikake furikake think add much value entire dish mostly tasted togarashi japanese chili powder onsen egg little disappointed yolk already brokeoverall average tasting bowl beef felt could given slightly meat maybe add furikake rice instead meat least people eat rice taste</th>\n",
       "      <td>taste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>folks fukudonsg got mise en place kitchen line tee enabling serve steady stream customers quickly doesn t always work favour beef tender unctuous onsen egg yolk already overcooked nevertheless spot flavours although always rice bowls could definitely saucetaste</th>\n",
       "      <td>bowls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id hardpressed find another place serving salmon quality price point budget friendly yet tasty donburis queue starts forming way shutters go lunch everyday nooneach serving comes pieces succulent oily salmon coated tangy shoyu lemon glaze side fukudonsg japanese tomato salad bed al dente calrose rice portion small every bite flavour packed want get hands come it s one bestsellers run inevitablytaste</th>\n",
       "      <td>flavour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asn t intending settle dinner marine parade food centre initially pretty bummed fact spot intended dine area many items menu unavailable   thus found heading food centre checking fukudon describes asian fusion donburi specialist following social media quite whilefelt  asian fusion donburi  branding bit far stretch given various donburi offered menu ie chicken breast oyakodon shoyu glaze salmon sukiyaki beef short plate onsen eg etc sound pretty japaneseinspired   said impressed pork belly katsu opted fried pork belly seems carry bite less gelatinous fatty initially expected especially given illustrations menu element sealed deal us runny egg simmered dashi broth alongside onions   incredibly runny silky eggs also carried punchy savoury note provides much flavour entire dish sauce even permeating bed shortgrain rice gives otherwise plain rice moisture sweetsavoury note japanese spicy powder also sprinkled top provide donburi slight hint spiciness   enough tickle tastebuds without overwhelming entire dishgiven tried fukudon left pretty impressed offer   execution eggs particularly noteworthy pretty much important element makes good oyakodon katsudon it s offerings seem rather japanese heart seem places quite bit effort trying serve quality food masses pocketfriendly prices quite interested thunder crunchy fries comes either homemade curry sauce mentaiko sauce   something pretty keen try subsequent visit</th>\n",
       "      <td>visit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw new stall marine parade food centre operating bunch young people serving donburi food courtalthough opened recently attracted long queue peak hoursgo signature pork belly katsu topped slices thick crispy juicy pork katsu simmered together egg onion japanese ricethe portion wasn t big sufficient</th>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>howeverii wish least sauce go along plentiful rice</th>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>onsen egg yolk already overcooked</th>\n",
       "      <td>yolk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fried pork belly seems carry bite less gelatinous fatty initially expected especially given illustrations menu</th>\n",
       "      <td>menu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serving comes pieces succulent</th>\n",
       "      <td>pieces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>makes little special translucent strings white onion lightly pickled take away rawness</th>\n",
       "      <td>onion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rice bowls could definitely sauce</th>\n",
       "      <td>bowls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food kinda disappointing compared good reviews written online value terms taste price wont queuing againthe issue service staff poor attitude taking customers orderrequests seem trained comes food handling multiple occasions saw staff scratching hair handling customers food wearing mask properly</th>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chicken cold new establishment ok always room improvements mainly poor attitude rude girl taking order told food cold stares u terrible especially us work service industry could nicely communicated chose act snobbishwould given stars food star person taking order thus star rating</th>\n",
       "      <td>star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>could given possibleextremely disappointed pork belly katsu beef donboth dishes fell way expectations expect food lookeven similar shown menukatsu soggy beef toughwill visit againpricing wise tastier food marine parade hawker centre spare change</th>\n",
       "      <td>centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always wanted try open noon severely limited ability eat earlier cos prefer eat lunch tried spicy pork collar  did look nice photos incredibly yellow curry powder primary ingredient pork would expected drizzled spice powders knew curry powder would ordered thisthe meat dishes fried spot pork shreds properly separated ended stuck together clumped together worried uncooked betweenthe called onsen egg    video could put little egg yolk oozed rest semi hardsemi generous spring onions wish gave bit even tho asked bit garnishing since fusion fried garlic chipscrumbs even shallotsunlikely go back try dishes</th>\n",
       "      <td>dishes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recommended family come try first always skeptical see sprinkles furikake powders top bowl japanese rarely mostly skews taste meat rice portion decent worst rice never seen bad rice even talk japanese rice dry brittle hell felt like overnight rice something tried understanding man super disappointing</th>\n",
       "      <td>rice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>didn t enjoy food egg sweet pork katsu flat waste calories</th>\n",
       "      <td>katsu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nothing special ordered beef short plate slight tough chew good value served bowl full rice little meats hoping quality would outweigh quantity seems lacking young lads hardworking though</th>\n",
       "      <td>quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad interesting new generation food queue pretty long worth wait portion small nothing special size price high side</th>\n",
       "      <td>side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new stall hawker centre chicken karaage curry sauce rice impressed huge portion price left place feeling extremely full chicken served piping hot thigh meat well marinated tender layered well seasoned crisp light batterthe curry sauce interesting expected japanese curry seemed like rendition mcdonald s curry sauce pineappley version itthe additional side potato salad though extremely disappointing contributed reduction stars review serving size also larger expected seemed like onion salad disproportionately large amount almostraw crunchy onions salad ruined entire dishoverall hits misses would rate stars worth giving chance you re area</th>\n",
       "      <td>area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rice looks like shortgrain tastes like rice get average cai fan stall sukiyaki beef okay terms pricetomeat ratio beef strips pretty tender onsen egg texture also point unfortunately really hated rice quality back spicy marinated pork collar wasnt great us socalled pork collar actually made minced pork didnt flavour texture expecting given price point forgiveable also nothing draw back unless dabao onsen eggs la carte</th>\n",
       "      <td>eggs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got sukiyaki beef egg stated onions somehow think ate tasted anythe rice bad drenched beef drippings added flavour plain ricefurikake placed top meat didnt really add much value dish tasted togarashi powder japanese chili powderbeef tender could generous amount given teriyaki sauce okay tasted like befor onsen egg little disappointed yolk already brokeoverall average tasting constant queue customer</th>\n",
       "      <td>queue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice bowl katsudon felt bit salty side wasn t anything special price reasonable given small portion worth mins wait maybe may tempted queue shorter</th>\n",
       "      <td>queue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hits misses good first time tried short beef impressed trying pork coller average hv go</th>\n",
       "      <td>pork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thought way salty really quite average tastewise seems overpriced</th>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>located marine parade hawker stall specializes donswe sukiyaki katsu good soft boiled eggs perfectly cooked flavours point wasnt expecting hawker stall food reasonably priced tooexpect queue going peak hours place accepts cash</th>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travelled jurong marine parade try hawker asian fusion donburi specialist read many rave reviews recommendations foodiesfukudon marine parade central market food centrefukudonsgcurrent christmas promotionmarine parade outlet donburi netton christmas eve ordered takeaway chicken breast oyakodon katsu platter shoyu glaze salmon spicy garlic pork collar wonsen egg sukiyaki beef short plate w onsen eggx sets eachoperating hoursclosed mondaytue fri sat sun verdictdefinitely must try worth queuing</th>\n",
       "      <td>onsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ent order pork katsudon today long queue placed order went back table went pick order saw already plastic bag rushed collect order went back home realized home two packages instead one turned took wrong order someone else smy two daughters liked beef salmon order accidentally took tried contact whatsapp response fast understood asked come back return food keep house happy order service</th>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place hidden gem fantastic dons reasonable prices great value money back soon</th>\n",
       "      <td>money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pork belly i ve tasted expensive ones aren t good you re talking value right nom nom nom</th>\n",
       "      <td>nom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ordered pork belly katsu bowl nothing complain try another time</th>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sukiyaki beef short plate onsen eg etc</th>\n",
       "      <td>eg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>felt  asian fusion donburi  branding bit far stretch given various donburi offered menu</th>\n",
       "      <td>menu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Aspect\n",
       "super valued money super affordable yet restaur...      beef\n",
       "place certainly worthy tried pork katsu beef fe...    outlet\n",
       "picture doesn t food justice fav place pork spi...  takeaway\n",
       "marine parade saw stall selling japanese consta...     taste\n",
       "folks fukudonsg got mise en place kitchen line ...     bowls\n",
       "id hardpressed find another place serving salmo...   flavour\n",
       "asn t intending settle dinner marine parade foo...     visit\n",
       "saw new stall marine parade food centre operati...         t\n",
       "howeverii wish least sauce go along plentiful rice      rice\n",
       "onsen egg yolk already overcooked                       yolk\n",
       "fried pork belly seems carry bite less gelatino...      menu\n",
       "serving comes pieces succulent                        pieces\n",
       "makes little special translucent strings white ...     onion\n",
       "rice bowls could definitely sauce                      bowls\n",
       "food kinda disappointing compared good reviews ...      food\n",
       "chicken cold new establishment ok always room i...      star\n",
       "could given possibleextremely disappointed pork...    centre\n",
       "always wanted try open noon severely limited ab...    dishes\n",
       "recommended family come try first always skepti...      rice\n",
       "didn t enjoy food egg sweet pork katsu flat was...     katsu\n",
       "nothing special ordered beef short plate slight...   quality\n",
       "bad interesting new generation food queue prett...      side\n",
       "new stall hawker centre chicken karaage curry s...      area\n",
       "rice looks like shortgrain tastes like rice get...      eggs\n",
       "got sukiyaki beef egg stated onions somehow thi...     queue\n",
       "nice bowl katsudon felt bit salty side wasn t a...     queue\n",
       "hits misses good first time tried short beef im...      pork\n",
       "thought way salty really quite average tastewis...       way\n",
       "located marine parade hawker stall specializes ...     place\n",
       "travelled jurong marine parade try hawker asian...     onsen\n",
       "ent order pork katsudon today long queue placed...   service\n",
       "place hidden gem fantastic dons reasonable pric...     money\n",
       "pork belly i ve tasted expensive ones aren t go...       nom\n",
       "ordered pork belly katsu bowl nothing complain ...      time\n",
       "sukiyaki beef short plate onsen eg etc                    eg\n",
       "felt  asian fusion donburi  branding bit far st...      menu"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90946981-63e4-49ae-bb7b-96bda3e88e1d",
   "metadata": {},
   "source": [
    "### 2. Create a dict then df for aspect-descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "fca1c217-0028-491f-9561-70b2835172fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aim to create a asepct-descriptive term df \n",
    "\n",
    "# initialisation\n",
    "temp_neg_asp_descrp_dict = {}\n",
    "\n",
    "# code \n",
    "for sentence in neg_reviews_list:\n",
    "    s = clean_text(sentence)\n",
    "    doc = nlp(s)\n",
    "    descriptive_term = ''\n",
    "    aspects = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ' :\n",
    "            descriptive_term = str(token)\n",
    "        if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" or token.pos_ == \"VERB\":\n",
    "            asp = str(token)\n",
    "            aspects.append(str(token)) #for each sentence, store all the aspects in a list \n",
    "            \n",
    "        if asp in top_neg_aspects:\n",
    "            \n",
    "            if descriptive_term != '':\n",
    "                \n",
    "                if asp not in temp_neg_asp_descrp_dict:\n",
    "                    temp_neg_asp_descrp_dict[asp] = descriptive_term\n",
    "                    \n",
    "                else:\n",
    "                    temp_neg_asp_descrp_dict[asp] += ',' + descriptive_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8fd6f333-e184-458a-a70e-13bd58901574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>affordable,affordable,great,great,great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sukiyaki</th>\n",
       "      <td>standard,shoyu,average,central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beef</th>\n",
       "      <td>standard,worthy,worthy,constant,short,short,sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>worthy,huge,soft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pork</th>\n",
       "      <td>worthy,impressed,impressed,long,thick,fried,di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amount</th>\n",
       "      <td>large,generous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture</th>\n",
       "      <td>average,average,socalled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>christmas</th>\n",
       "      <td>central,central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nom</th>\n",
       "      <td>right,right,right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Description\n",
       "money                affordable,affordable,great,great,great\n",
       "sukiyaki                      standard,shoyu,average,central\n",
       "beef       standard,worthy,worthy,constant,short,short,sh...\n",
       "place                                       worthy,huge,soft\n",
       "pork       worthy,impressed,impressed,long,thick,fried,di...\n",
       "...                                                      ...\n",
       "amount                                        large,generous\n",
       "texture                             average,average,socalled\n",
       "time                                                   first\n",
       "christmas                                    central,central\n",
       "nom                                        right,right,right\n",
       "\n",
       "[109 rows x 1 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_asp_descrp_df = pd.DataFrame.from_dict(temp_neg_asp_descrp_dict, orient='index',columns=['Description'])\n",
    "neg_asp_descrp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41edfc9f-0045-488e-883a-927a33d850be",
   "metadata": {},
   "source": [
    "### NEGATIVE VISUALISATION\n",
    "\n",
    "\n",
    "Use plotly ==> https://plotly.com/python/getting-started/ \n",
    "For each aspect, get unique description words (x-axis) and the count(y-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e51cb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put description as a list for each aspect in top_5_neg_aspects:\n",
    "neg_asp_dict = {}\n",
    "for asp in temp_neg_asp_descrp_dict:\n",
    "    content = temp_neg_asp_descrp_dict[asp]\n",
    "    to_list = content.split(',')\n",
    "    neg_asp_dict[asp] = to_list\n",
    "\n",
    "# #combine 2 similar words HERE\n",
    "# neg_asp_dict['mentaiko'] += neg_asp_dict['mentai']\n",
    "# del neg_asp_dict['mentai']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49727300",
   "metadata": {},
   "source": [
    "#### Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f51d3f-2178-4a4e-b141-cd33ded6f4bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for asp in neg_asp_dict:\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for x,y in zip(unique_words(neg_asp_dict,asp),get_counts_desc_list(neg_asp_dict, asp)):\n",
    "        if y>1:\n",
    "            x_list.append(x)\n",
    "            y_list.append(y)\n",
    "    if len(x_list)!=0:\n",
    "        fig = px.bar(x= x_list, y= y_list, title=\"Negative: \" + asp, labels=dict(x =\"Description\",y= \"Frequency\"))\n",
    "        fig.update_layout(xaxis={'categoryorder':'total descending'})\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e53ed1",
   "metadata": {},
   "source": [
    "#### PIe Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7a5a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for asp in neg_asp_dict:\n",
    "    df = pd.DataFrame(unique_words(neg_asp_dict,asp), columns=['Description'])\n",
    "    df['counts'] = get_counts_desc_list(neg_asp_dict, asp)\n",
    "    fig = px.pie(df, values= 'counts', names='Description', title='Negative: '+ asp)\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
